{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task6_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjtA8Zt5AOdl",
        "outputId": "9ccf653c-1890-43f1-b298-527863d6db6e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import nltk\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from joblib import dump\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "np.random.seed(500) #used to reproduce the same result every time\n",
        "!git clone https://github.com/bieli/stopwords.git  #repository with polish stopwords "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "fatal: destination path 'stopwords' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EDjeewmAwr8"
      },
      "source": [
        "def load_data(path: str)-> list:\n",
        "  dataset = []\n",
        "  with open(path) as file:\n",
        "    for line in file:\n",
        "      dataset.append(line)\n",
        "  return dataset \n",
        "\n",
        "def prepare_and_clean_data(text: list, labels: list):\n",
        "  df1 = {'label': labels, 'text': text}\n",
        "  data = pd.DataFrame(df1)\n",
        "  data = data.replace('\\n','', regex=True)\n",
        "  data[\"label\"] = pd.to_numeric(data[\"label\"])\n",
        "  #remove @annonymized_account \n",
        "  data = data.replace('@anonymized_account','', regex=True)\n",
        "  return data"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B69NtbsZIPAM"
      },
      "source": [
        "text = load_data(\"./training_set_clean_only_text.txt\")\n",
        "labels = load_data(\"./training_set_clean_only_tags.txt\")\n",
        "data = prepare_and_clean_data(text,labels)\n",
        "text_test = load_data(\"./test_set_only_text.txt\")\n",
        "labels_test = load_data(\"./test_set_only_tags.txt\")\n",
        "data_test = prepare_and_clean_data(text_test,labels_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAtx8BeWII39"
      },
      "source": [
        "def process_data(data: pd.DataFrame, stopwords_path: str):\n",
        "  # remove blank rows.\n",
        "  data['text'].dropna(inplace=True)\n",
        "  # Change text to lower case\n",
        "  data['text'] = [entry.lower() for entry in data['text']]\n",
        "  # Tokenization \n",
        "  data['text']= [word_tokenize(entry) for entry in data['text']]\n",
        "  # Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
        "  # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
        "  tag_map = defaultdict(lambda : wn.NOUN)\n",
        "  tag_map['J'] = wn.ADJ\n",
        "  tag_map['V'] = wn.VERB\n",
        "  tag_map['R'] = wn.ADV\n",
        "  stopwords = open(stopwords_path,'r').read().split('\\n')\n",
        "  for index,entry in enumerate(data['text']):\n",
        "    Final_words = []\n",
        "    word_Lemmatized = WordNetLemmatizer()\n",
        "    # pos_tag function below will provide the 'tag', if the word is N, V, ADV, ADJ\n",
        "    for word, tag in pos_tag(entry):\n",
        "        #check for Stop words and consider only alphabets\n",
        "        if word not in stopwords and word.isalpha():\n",
        "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
        "            Final_words.append(word_Final)\n",
        "    data.loc[index,'text_final'] = str(Final_words)\n",
        "  return data"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AteC9jklKxXO"
      },
      "source": [
        "data = process_data(data,\"./stopwords/polish.stopwords.txt\")\n",
        "data_test = process_data(data_test,\"./stopwords/polish.stopwords.txt\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ED_mUNZLFmP"
      },
      "source": [
        "train_X = data['text_final'] \n",
        "train_y = data['label']\n",
        "test_X = data_test['text_final']\n",
        "test_y = data_test['label']\n",
        "\n",
        "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "Tfidf_vect.fit(data['text_final'])\n",
        "Train_X_Tfidf = Tfidf_vect.transform(train_X)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(test_X)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyw_jqwVqAj_",
        "outputId": "95e5ee10-765d-46ff-ae81-84c73f018c89"
      },
      "source": [
        "oversample = SMOTE(random_state=42)\n",
        "\n",
        "Train_X_Tfidf,train_y = oversample.fit_resample(Train_X_Tfidf,train_y)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FHbio1VjyCo",
        "outputId": "39fce76b-3a12-440d-d31e-abb8bf922c54"
      },
      "source": [
        "model = RandomForestClassifier()\n",
        "# fit the model on the whole dataset\n",
        "model.fit(Train_X_Tfidf,train_y)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7dYwbQ0kYLG",
        "outputId": "d7a39005-760d-4e00-c730-59ca92949348"
      },
      "source": [
        "RFC_pred = model.predict(Test_X_Tfidf)\n",
        "print(classification_report(test_y, RFC_pred))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90       866\n",
            "           1       0.08      0.12      0.09        25\n",
            "           2       0.38      0.33      0.35       109\n",
            "\n",
            "    accuracy                           0.82      1000\n",
            "   macro avg       0.45      0.45      0.45      1000\n",
            "weighted avg       0.83      0.82      0.82      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtqRRPMEL36m",
        "outputId": "c51ee4ce-5116-4444-c07e-7c7d78db3823"
      },
      "source": [
        "dump(model,'task6model.pkl')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['task6model.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiBdPfZSOrJt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}